{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### University of Virginia\n",
    "### DS 5559: Big Data Analytics\n",
    "\n",
    "### Classification of Wisconsin Breast Cancer Database\n",
    "### Last updated: Feb 20, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions** \n",
    "\n",
    "In this project, you will work with the Wisconsin Breast Cancer dataset.  You will train a logistic regression model to predict the diagnosis.  First, you will work through this example.  Then you will make modifications and run the code, collecting results at the bottom of the notebook.\n",
    "\n",
    "The following experiments should be conducted:\n",
    "1.  Three features were used in the model.  Build the  model using all features.\n",
    "Before training the model, apply scaling to the features using the StandardScaler\n",
    "transformer.  Then train the model and compute the accuracy on the test set.  Additionally, compute and show the confusion matrix.\n",
    "\n",
    "2. Repeat step (1), including an intercept\n",
    "3. Repeat step (1), using randomSplit([0.7, 0.3])\n",
    "4. Repeat step (2), using randomSplit([0.7, 0.3])\n",
    "5. Compare and discuss the results of (1) vs (2).  Compare and discuss the results of (3) vs (4).\n",
    "\n",
    "**Total Possible Points: 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql.functions import col \n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"data preprocessing\") \\\n",
    "    .config(\"spark.executor.memory\", '8g') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.cores.max', '4') \\\n",
    "    .config(\"spark.driver.memory\",'8g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.driver.port', '39481'),\n",
       " ('spark.driver.host', 'udc-ba26-10'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'data preprocessing'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.cores.max', '4'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1591652769640'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.memory', '8g'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "data_file = \"wisc_breast_cancer_w_fields.csv\"\n",
    "df = spark.read.csv(data_file, inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use some of the fields as features\n",
    "assembler = VectorAssembler(inputCols=[\"f1\", \"f2\", \"f3\"], outputCol=\"features\") \n",
    "transformed = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to RDD\n",
    "dataRdd_s = transformed.select(col(\"diagnosis\"), col(\"features\")).rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', DenseVector([17.99, 10.38, 122.8])),\n",
       " ('M', DenseVector([20.57, 17.77, 132.9]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRdd_s.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to RDD\n",
    "dataRdd = transformed.select(col(\"diagnosis\"), col(\"features\")).rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', DenseVector([17.99, 10.38, 122.8])),\n",
       " ('M', DenseVector([20.57, 17.77, 132.9]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at some data\n",
    "dataRdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map label to binary values, then convert to LabeledPoint\n",
    "lp = dataRdd.map(lambda row:(1 if row[0]=='M' else 0, Vectors.dense(row[1])))    \\\n",
    "                    .map(lambda row: LabeledPoint(row[0], row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [17.99,10.38,122.8]),\n",
       " LabeledPoint(1.0, [20.57,17.77,132.9])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at some data\n",
    "lp.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data approximately into training (60%) and test (40%)\n",
    "training, test = lp.randomSplit([0.6, 0.4], seed=314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356, 213, 569)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count records in datasets\n",
    "(training.count(), test.count(), lp.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6256590509666081, 0.37434094903339193, 1.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(training.count()/lp.count(), test.count()/lp.count(), lp.count()/lp.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy (test): 0.8732394366197183\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on test data\n",
    "labelsAndPreds_te = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "accuracy_te = 1.0 * labelsAndPreds_te.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy (test): {}'.format(accuracy_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTIONS**  \n",
    " For parts 1-4, compute and show for the test set: (1) accuracy (2) confusion matrix.  \n",
    " Each part is worth 2 POINTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+\n",
      "|Summary|                f1|               f2|               f3|\n",
      "+-------+------------------+-----------------+-----------------+\n",
      "|  count|               569|              569|              569|\n",
      "|   mean|14.127291739894563|19.28964850615117|91.96903339191566|\n",
      "| stddev|3.5240488262120793|4.301035768166948| 24.2989810387549|\n",
      "|    min|             6.981|             9.71|            43.79|\n",
      "|    max|             28.11|            39.28|            188.5|\n",
      "+-------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().select(\"Summary\", \"f1\", \"f2\", \"f3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           features|\n",
      "+-------------------+\n",
      "|[17.99,10.38,122.8]|\n",
      "|[20.57,17.77,132.9]|\n",
      "|[19.69,21.25,130.0]|\n",
      "|[11.42,20.38,77.58]|\n",
      "|[20.29,14.34,135.1]|\n",
      "| [12.45,15.7,82.57]|\n",
      "|[18.25,19.98,119.6]|\n",
      "| [13.71,20.83,90.2]|\n",
      "|  [13.0,21.82,87.5]|\n",
      "|[12.46,24.04,83.97]|\n",
      "|[16.02,23.24,102.7]|\n",
      "|[15.78,17.89,103.6]|\n",
      "| [19.17,24.8,132.4]|\n",
      "|[15.85,23.95,103.7]|\n",
      "| [13.73,22.61,93.6]|\n",
      "|[14.54,27.54,96.73]|\n",
      "|[14.68,20.13,94.74]|\n",
      "|[16.13,20.68,108.1]|\n",
      "|[19.81,22.15,130.0]|\n",
      "|[13.54,14.36,87.46]|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"f1\", \"f2\", \"f3\"], outputCol=\"features\") \n",
    "# Now let us use the transform method to transform our dataset\n",
    "df=assembler.transform(df)\n",
    "df.select(\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|           features|     Scaled_features|\n",
      "+-------------------+--------------------+\n",
      "|[17.99,10.38,122.8]|[5.10492359418783...|\n",
      "|[20.57,17.77,132.9]|[5.83703603849048...|\n",
      "|[19.69,21.25,130.0]|[5.58732326679035...|\n",
      "|[11.42,20.38,77.58]|[3.24059074183574...|\n",
      "|[20.29,14.34,135.1]|[5.75758197476771...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "standardscaler=StandardScaler().setInputCol(\"features\").\\\n",
    "setOutputCol(\"Scaled_features\")\n",
    "df=standardscaler.fit(df).transform(df)\n",
    "df.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to RDD\n",
    "dataRdd_s = transformed.select(col(\"diagnosis\"), col(\"features\")).rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enter solution for Part 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enter solution for Part 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enter solution for Part 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enter solution for Part 5\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
